{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19572,"status":"ok","timestamp":1648513123125,"user":{"displayName":"myles cullen","userId":"07229132477989411836"},"user_tz":-60},"id":"ytxqnVy27a4c","outputId":"e37e9bda-6efe-47cc-b659-abf5f39a8703"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43508,"status":"ok","timestamp":1648513166630,"user":{"displayName":"myles cullen","userId":"07229132477989411836"},"user_tz":-60},"id":"KB6U-f3h9ab7","outputId":"0fae517e-1559-4c42-a9db-89df2f4d3205"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-gpu==1.15\n","  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n","\u001b[K     |████████████████████████████████| 411.5 MB 5.6 kB/s \n","\u001b[?25hRequirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.1)\n","Requirement already satisfied: google-pasta\u003e=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n","Requirement already satisfied: grpcio\u003e=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.44.0)\n","Collecting keras-applications\u003e=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n","Collecting tensorboard\u003c1.16.0,\u003e=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 63.7 MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing\u003e=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 65.7 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf\u003e=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n","Requirement already satisfied: absl-py\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.0.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: numpy\u003c2.0,\u003e=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.21.5)\n","Requirement already satisfied: astor\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n","Requirement already satisfied: six\u003e=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n","Requirement already satisfied: wrapt\u003e=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.14.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications\u003e=1.0.8-\u003etensorflow-gpu==1.15) (3.1.0)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.16.0,\u003e=1.15.0-\u003etensorflow-gpu==1.15) (1.0.1)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.16.0,\u003e=1.15.0-\u003etensorflow-gpu==1.15) (3.3.6)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c1.16.0,\u003e=1.15.0-\u003etensorflow-gpu==1.15) (57.4.0)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c1.16.0,\u003e=1.15.0-\u003etensorflow-gpu==1.15) (4.11.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.16.0,\u003e=1.15.0-\u003etensorflow-gpu==1.15) (3.7.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.16.0,\u003e=1.15.0-\u003etensorflow-gpu==1.15) (3.10.0.2)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py-\u003ekeras-applications\u003e=1.0.8-\u003etensorflow-gpu==1.15) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=b186ef2c47419d4252691b996390d0ad19e37024210c233470058f4ae933756f\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires tensorboard\u003c2.9,\u003e=2.8, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow-probability 0.16.0 requires gast\u003e=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"]}],"source":["!pip install tensorflow-gpu==1.15"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FNqnEvg72Rv","outputId":"584fd6e8-a4b1-4ce3-845a-f31c844c5099"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-03-29 00:19:27--  https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/train.tar.gz\n","Resolving mycityreport.s3-ap-northeast-1.amazonaws.com (mycityreport.s3-ap-northeast-1.amazonaws.com)... 52.219.17.46\n","Connecting to mycityreport.s3-ap-northeast-1.amazonaws.com (mycityreport.s3-ap-northeast-1.amazonaws.com)|52.219.17.46|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1472626254 (1.4G) [application/x-tar]\n","Saving to: ‘train.tar.gz’\n","\n","train.tar.gz         92%[=================\u003e  ]   1.27G  8.19MB/s    eta 9s     "]}],"source":["!wget https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/train.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn49aFJs75R_"},"outputs":[],"source":["\n","!tar -xzvf  'train.tar.gz'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hhxiEQ18i1M"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.contrib as tf_contrib\n","from tensorflow.contrib.layers import variance_scaling_initializer as he_init\n","\n","\n","##################################################################################\n","# Layer\n","##################################################################################\n","\n","def linear(x, units, use_bias=True, activation_fn='leaky', is_training=True, norm_fn='instance', scope='linear'):\n","    with tf.variable_scope(scope):\n","        x = tf.layers.flatten(x)\n","        x = tf.layers.dense(x, units=units, kernel_initializer=tf_contrib.layers.variance_scaling_initializer(), kernel_regularizer=tf_contrib.layers.l2_regularizer(scale=0.0001), use_bias=use_bias)\n","\n","        if norm_fn == 'instance' :\n","            x = instance_norm(x, 'instance')\n","        if norm_fn == 'batch' :\n","            x = batch_norm(x, is_training, 'batch')\n","        \n","        if activation_fn!='None':\n","            x = activation(x, activation_fn)\n","        return x\n","                \n","def conv(x, channels, kernel=3, stride=2, pad=0, activation_fn='leaky', is_training=True, norm_fn='instance', is_spectral_norm=False, use_bias=True, scope='conv_0') :\n","    with tf.variable_scope(scope):\n","        if scope.__contains__(\"discriminator\") :\n","            weight_init = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n","        else :\n","            weight_init = tf_contrib.layers.xavier_initializer()\n","\n","        x = tf.pad(x, [[0,0], [pad, pad], [pad, pad], [0,0]])\n","        \n","        if is_spectral_norm == True :\n","            w = tf.get_variable(\"kernel\", shape=[kernel, kernel, x.get_shape()[-1], channels], initializer=weight_init,\n","                                regularizer=None)\n","            bias = tf.get_variable(\"bias\", [channels], initializer=tf.constant_initializer(0.0))\n","            x = tf.nn.conv2d(input=x, filter=spectral_norm(w),\n","                             strides=[1, stride, stride, 1], padding='VALID')\n","            if use_bias:\n","                x = tf.nn.bias_add(x, bias)\n","\n","        else :\n","            x = tf.layers.conv2d(inputs=x, filters=channels,\n","                                 kernel_size=kernel, kernel_initializer=weight_init,\n","                                 kernel_regularizer=tf_contrib.layers.l2_regularizer(scale=0.0001),\n","                                 strides=stride, use_bias=use_bias)\n","\n","        if norm_fn == 'instance' :\n","            x = instance_norm(x, 'instance')\n","        if norm_fn == 'batch' :\n","            x = batch_norm(x, is_training, 'batch')\n","        if norm_fn == 'layer' :\n","            x = layer_norm(x, scope='layer_norm') \n","\n","        if activation_fn!='None':\n","            x = activation(x, activation_fn)\n","            \n","        return x\n","\n","def deconv(x, channels, kernel=4, stride=2, activation_fn='leaky', is_training=True, norm_fn='instance', use_bias=True, scope='deconv_0'):\n","    with tf.variable_scope(scope):\n","        weight_init = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n","        x = tf.layers.conv2d_transpose(inputs=x, filters=channels,\n","                                       kernel_size=kernel, kernel_initializer=weight_init, \n","                                       kernel_regularizer=tf_contrib.layers.l2_regularizer(scale=0.0001),\n","                                       strides=stride, padding='SAME', use_bias=use_bias)\n","\n","        if norm_fn == 'instance' :\n","            x = instance_norm(x, 'instance')\n","        if norm_fn == 'batch' :\n","            x = batch_norm(x, is_training, 'batch')\n","        if norm_fn == 'layer' :\n","            x = layer_norm(x, scope='layer_norm') \n","\n","        if activation_fn!='None':\n","            x = activation(x, activation_fn)\n","\n","        return x\n","    \n","def gaussian_noise_layer(mu, scope='noise'):\n","    with tf.variable_scope(scope):\n","        sigma = 0.4#tf.get_variable(\"sigma\", [1], tf.float32, tf.random_normal_initializer(stddev=0.02))\n","        gaussian_random_vector = tf.random_normal(shape=tf.shape(mu), mean=0.0, stddev=1.0, dtype=tf.float32)\n","        return mu + sigma * gaussian_random_vector\n","\n","##################################################################################\n","# Residual-block\n","##################################################################################\n","\n","def resblock(x_init, channels, kernel=3, stride=1, pad=1, dropout_ratio=0.0, is_training=True, norm_fn='instance', is_spectral_norm=False, use_bias=True, scope='resblock') :\n","    assert norm_fn in ['instance', 'batch', 'weight', 'spectral', 'None', 'layer']\n","    with tf.variable_scope(scope) :\n","        with tf.variable_scope('res1') :\n","            x = conv(x_init, channels, kernel=3, stride=1, pad=1, activation_fn='relu', norm_fn=norm_fn, is_spectral_norm=is_spectral_norm, use_bias=use_bias) \n","            \n","        with tf.variable_scope('res2') :\n","            x = conv(x, channels, kernel=3, stride=1, pad=1, activation_fn='None', norm_fn=norm_fn, is_spectral_norm=is_spectral_norm, use_bias=use_bias) \n","\n","        if dropout_ratio \u003e 0.0 :\n","            x = tf.layers.dropout(x, rate=dropout_ratio, training=is_training)\n","\n","        return x + x_init\n","\n","def adaptive_resblock(x_init, channels, mu, sigma, is_spectral_norm=False, use_bias=True, scope='adaptive_resblock') :\n","    with tf.variable_scope(scope):\n","        with tf.variable_scope('res1'):\n","            x = conv(x_init, channels, kernel=3, stride=1, pad=1, activation_fn='None', norm_fn='None', is_spectral_norm=is_spectral_norm, use_bias=use_bias) \n","            x = adaptive_instance_norm(x, mu, sigma)\n","            x = relu(x)\n","\n","        with tf.variable_scope('res2'):\n","            x = conv(x, channels, kernel=3, stride=1, pad=1, activation_fn='None', norm_fn='None', is_spectral_norm=is_spectral_norm, use_bias=use_bias) \n","            x = adaptive_instance_norm(x, mu, sigma)\n","\n","        return x + x_init\n","    \n","def hw_flatten(x):\n","    return tf.reshape(x, shape=[x.shape[0], -1, x.shape[-1]])\n","\n","##################################################################################\n","# Activation function\n","##################################################################################\n","        \n","def activation(x, activation_fn='leaky') :\n","    assert activation_fn in ['relu', 'leaky', 'tanh', 'sigmoid', 'swish', 'selu', 'None']\n","    if activation_fn == 'leaky':\n","        x = lrelu(x)\n","\n","    if activation_fn == 'relu':\n","        x = relu(x)\n","\n","    if activation_fn == 'sigmoid':\n","        x = sigmoid(x)\n","\n","    if activation_fn == 'tanh' :\n","        x = tanh(x)\n","\n","    if activation_fn == 'swish' :\n","        x = swish(x)\n","\n","    if activation_fn == 'selu' :\n","        x = selu(x)\n","\n","    return x\n","\n","def lrelu(x, alpha=0.01) :\n","    # pytorch alpha is 0.01\n","    return tf.nn.leaky_relu(x, alpha)\n","\n","def relu(x) :\n","    return tf.nn.relu(x)\n","\n","def sigmoid(x) :\n","    return tf.sigmoid(x)\n","\n","def tanh(x) :\n","    return tf.tanh(x)\n","\n","def swish(x) :\n","    return x * sigmoid(x)\n","\n","def selu(x) :\n","    return tf.nn.selu(x)\n","\n","##################################################################################\n","# Normalization function\n","##################################################################################\n","\n","def batch_norm(x, is_training=False, scope='batch_nom') :\n","    return tf_contrib.layers.batch_norm(x,\n","                                        decay=0.9, epsilon=1e-05,\n","                                        center=True, scale=True, updates_collections=None,\n","                                        is_training=is_training, scope=scope)\n","\n","def instance_norm(x, scope='instance_nom') :\n","    return tf_contrib.layers.instance_norm(x,\n","                                           epsilon=1e-05,\n","                                           center=True, scale=True,\n","                                           scope=scope)\n","\n","def layer_norm(x, scope='layer_norm') :\n","    return tf_contrib.layers.layer_norm(x,\n","                                        center=True, scale=True,\n","                                        scope=scope)\n","\n","def adaptive_instance_norm(content, gamma, beta, epsilon=1e-5):\n","    # gamma, beta = style_mean, style_std from MLP\n","\n","    c_mean, c_var = tf.nn.moments(content, axes=[1, 2], keep_dims=True)\n","    c_std = tf.sqrt(c_var + epsilon)\n","\n","    return gamma * ((content - c_mean) / c_std) + beta\n","\n","def l2_norm(v, eps=1e-12):\n","    return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n","\n","def spectral_norm(w, iteration=1):\n","    w_shape = w.shape.as_list()\n","    w = tf.reshape(w, [-1, w_shape[-1]])\n","\n","    u = tf.get_variable(\"u\", [1, w_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)\n","\n","    u_hat = u\n","    v_hat = None\n","    for i in range(iteration):\n","        \"\"\"\n","        power iteration\n","        Usually iteration = 1 will be enough\n","        \"\"\"\n","        v_ = tf.matmul(u_hat, tf.transpose(w))\n","        v_hat = l2_norm(v_)\n","\n","        u_ = tf.matmul(v_hat, w)\n","        u_hat = l2_norm(u_)\n","\n","    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n","    w_norm = w / sigma\n","\n","    with tf.control_dependencies([u.assign(u_hat)]):\n","        w_norm = tf.reshape(w_norm, w_shape)\n","\n","    return w_norm\n","\n","##################################################################################\n","# Sampling\n","##################################################################################\n","\n","def down_sample(x) :\n","    return tf.layers.average_pooling2d(x, pool_size=3, strides=2, padding='SAME')\n","\n","def up_sample(x, scale_factor=2, metod='neighbor'):\n","    _, h, w, _ = x.get_shape().as_list()\n","    new_size = [h * scale_factor, w * scale_factor]\n","    if metod=='bilinear':\n","        return tf.image.resize_bilinear(x, size=new_size)\n","    else:\n","        return tf.image.resize_nearest_neighbor(x, size=new_size)\n","\n","def adaptive_avg_pooling(x):\n","    # global average pooling\n","    gap = tf.reduce_mean(x, axis=[1, 2], keep_dims=True)\n","\n","    return gap\n","\n","def KL_divergence(x) :\n","#    mu = tf.reduce_mean(x)\n","#    sigma_2 = tf.reduce_mean(tf.square(x-mu))\n","#    KL_divergence = 0.5 * (tf.square(mu) + sigma_2 - tf.log(sigma_2) - 1)\n","#    loss = tf.reduce_mean(KL_divergence)\n","\n","    loss = tf.square(tf.reduce_mean(x))\n","\n","    return loss\n","\n","##################################################################################\n","# Loss function\n","##################################################################################\n","\n","def regularization_loss(scope_name) :\n","    \"\"\"\n","    If you want to use \"Regularization\"\n","    g_loss += regularization_loss('generator')\n","    d_loss += regularization_loss('discriminator')\n","    \"\"\"\n","    collection_regularization = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","\n","    loss = []\n","    for item in collection_regularization :\n","        if scope_name in item.name :\n","            loss.append(item)\n","\n","    return tf.reduce_sum(loss)\n","\n","def L1_loss(x, y) :\n","    loss = tf.reduce_mean(tf.abs(x - y))\n","    return loss\n","\n","\n","def discriminator_loss(loss_func, real, fake):\n","    real_loss = 0\n","    fake_loss = 0\n","\n","    if loss_func.__contains__('wgan') :\n","        real_loss = -tf.reduce_mean(real)\n","        fake_loss = tf.reduce_mean(fake)\n","\n","    if loss_func == 'lsgan' :\n","        real_loss = tf.reduce_mean(tf.squared_difference(real, 1.0))\n","        fake_loss = tf.reduce_mean(tf.square(fake))\n","\n","    if loss_func == 'gan' or loss_func == 'dragan' :\n","        real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real), logits=real))\n","        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake), logits=fake))\n","\n","    if loss_func == 'hinge' :\n","        real_loss = tf.reduce_mean(relu(1.0 - real))\n","        fake_loss = tf.reduce_mean(relu(1.0 + fake))\n","\n","    loss = real_loss + fake_loss\n","\n","    return loss\n","\n","def generator_loss(loss_func, fake):\n","    fake_loss = 0\n","\n","    if loss_func.__contains__('wgan') :\n","        fake_loss = -tf.reduce_mean(fake)\n","\n","    if loss_func == 'lsgan' :\n","        fake_loss = tf.reduce_mean(tf.squared_difference(fake, 1.0))\n","\n","    if loss_func == 'gan' or loss_func == 'dragan' :\n","        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake), logits=fake))\n","\n","    if loss_func == 'hinge' :\n","        fake_loss = -tf.reduce_mean(fake)\n","\n","    loss = fake_loss\n","\n","    return loss\n","\n","def classification_loss1(logit, label) :\n","    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label, logits=logit))\n","    return loss\n","\n","def classification_loss2(logit, label) :\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit))\n","    return loss\n","\n","# def classification_loss2(label_o, label_p) :\n","#     loss = tf.reduce_mean(-label_o*tf.log(tf.clip_by_value(label_p,1e-10,1.0))-(1-label_o)*tf.log(tf.clip_by_value((1-label_p),1e-10,1.0)))\n","#     return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMkRlGSe-N0o"},"outputs":[],"source":["!pip install np_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1We2YLRB81P1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.contrib import slim\n","from scipy import misc\n","import os, random\n","import numpy as np\n","from glob import glob\n","import np_utils as np_utils\n","try:\n","    import xml.etree.cElementTree as ET  #解析xml的c语言版的模块\n","except ImportError:\n","    import xml.etree.ElementTree as ET\n","\n","class ImageData:\n","\n","    def __init__(self, data_path, img_shape=(64,64,1), augment_flag=False, data_type='None', img_type='jpg', pad_flag=False, label_size=8):\n","        self.data_path = data_path\n","        self.data_type = data_type\n","        self.img_shape = img_shape\n","        self.img_h = img_shape[0]\n","        self.img_w = img_shape[1]\n","        self.channels = img_shape[2]\n","        self.augment_flag = augment_flag\n","        self.img_type = img_type\n","        self.pad_flag = pad_flag\n","        self.label_size = label_size\n","        self.class_names = os.listdir(self.data_path)\n","        self.train_dataset = []\n","        self.train_label = []\n","        images = []\n","        for cl_name in self.class_names:\n","            img_names = os.listdir(os.path.join(self.data_path, cl_name))\n","            for img_name in img_names:\n","                self.train_dataset.append(os.path.abspath(os.path.join(self.data_path, cl_name, img_name)))\n","                hot_cl_name = self.get_class_one_hot(cl_name)\n","                self.train_label.append(hot_cl_name)\n","        self.train_label = np.reshape(self.train_label, (len(self.train_label), self.label_size))\n","\n","    def get_class_one_hot(self, class_str):\n","        label_encoded = self.class_names.index(class_str)\n","        label_hot = np_utils.to_categorical(label_encoded, len(self.class_names))\n","        label_hot = label_hot\n","\n","        return label_hot\n","\n","    def image_processing(self, filename, label):\n","        x = tf.read_file(filename)\n","        if self.img_type == 'jpg':\n","            x_decode = tf.image.decode_jpeg(x, channels=self.channels)\n","        if self.img_type == 'png':\n","            x_decode = tf.image.decode_png(x, channels=self.channels)\n","        if self.img_type == 'bmp':\n","            x_decode = tf.image.decode_bmp(x)\n","            if self.channels == 1 :\n","                x_decode = tf.image.rgb_to_grayscale(x_decode)\n","        img = tf.image.resize_images(x_decode, [self.img_h, self.img_w])\n","        img = tf.reshape(img, [self.img_h, self.img_w, self.channels])\n","        img = tf.cast(img, tf.float32) / 127.5 - 1\n","\n","        if self.augment_flag :\n","            img = tf.cond(pred=tf.greater_equal(tf.random_uniform(shape=[], minval=0.0, maxval=1.0), 0.5),\n","                          true_fn=lambda: augmentation(img),\n","                          false_fn=lambda: img)\n","        \n","        return img, label\n","    \n","def one_hot(batch_size, mask_size, location):\n","    l = tf.constant([location])\n","    m = tf.one_hot(l,mask_size,1.,0.)\n","    m = tf.tile(m,[batch_size,1])\n","    return m\n","    \n","def load_test_data(image_path, size_h=256, size_w=256):\n","    img = misc.imread(image_path, mode='RGB')\n","    img = misc.imresize(img, [size_h, size_w])\n","    img = np.expand_dims(img, axis=0)\n","    img = preprocessing(img)\n","\n","    return img\n","\n","def preprocessing(x):\n","    x = x/127.5 - 1 # -1 ~ 1\n","    return x\n","\n","def augmentation(image):\n","    seed = random.randint(0, 2 ** 31 - 1)\n","    image = tf.image.random_flip_left_right(image, seed=seed)\n","#    image = tf.image.random_brightness(image,max_delta=0.2)\n","#    image = tf.image.random_contrast(image, 0.5, 1.5)\n","#    image = tf.clip_by_value(image,-1.,1.)\n","#    image = tf.image.random_saturation(image, 0, 0.3)\n","    return image\n","\t\t\t\t\t\t\t\t\t\t\t         \n","def GetAnnotBoxLoc(AnotPath):#AnotPath VOC标注文件路径\n","    tree = ET.ElementTree(file=AnotPath)  #打开文件，解析成一棵树型结构\n","    root = tree.getroot()#获取树型结构的根\n","    ObjectSet=root.findall('object')#找到文件中所有含有object关键字的地方，这些地方含有标注目标\n","    ObjBndBoxSet={} #以目标类别为关键字，目标框为值组成的字典结构\n","    for Object in ObjectSet:\n","        ObjName=Object.find('name').text\n","        BndBox=Object.find('bndbox')\n","        x1 = int(BndBox.find('xmin').text)#-1 #-1是因为程序是按0作为起始位置的\n","        y1 = int(BndBox.find('ymin').text)#-1\n","        x2 = int(BndBox.find('xmax').text)#-1\n","        y2 = int(BndBox.find('ymax').text)#-1\n","        BndBoxLoc=[x1,y1,x2,y2]\n","        if ObjName in ObjBndBoxSet:\n","            ObjBndBoxSet[ObjName].append(BndBoxLoc)#如果字典结构中含有这个类别了，那么这个目标框要追加到其值的末尾\n","        else:\n","            ObjBndBoxSet[ObjName]=[BndBoxLoc]#如果字典结构中没有这个类别，那么这个目标框就直接赋值给其值吧\n","    return ObjBndBoxSet\n","\n","def save_images(images, size, image_path):\n","    return imsave(inverse_transform(images), size, image_path)\n","\n","def inverse_transform(images):\n","    return (images+1.) / 2\n","\n","def imsave(images, size, path):\n","    return misc.imsave(path, merge(images, size))\n","\n","def merge(images, size):\n","    h, w = images.shape[1], images.shape[2]\n","    img = np.zeros((h * size[0], w * size[1], 3))\n","    for idx, image in enumerate(images):\n","        i = idx % size[1]\n","        j = idx // size[1]\n","        img[h*j:h*(j+1), w*i:w*(i+1), :] = image\n","\n","    return img\n","\n","def show_all_variables():\n","    model_vars = tf.trainable_variables()\n","    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n","\n","def check_folder(log_dir):\n","    if not os.path.exists(log_dir):\n","        os.makedirs(log_dir)\n","    return log_dir\n","\n","def str2bool(x):\n","    return x.lower() in ('true')\n","\n","def summary(tensor_collection, summary_type=['mean', 'stddev', 'max', 'min', 'sparsity', 'histogram']):\n","    \"\"\"\n","    usage:\n","\n","    1. summary(tensor)\n","\n","    2. summary([tensor_a, tensor_b])\n","\n","    3. summary({tensor_a: 'a', tensor_b: 'b})\n","    \"\"\"\n","\n","    def _summary(tensor, name, summary_type=['mean', 'stddev', 'max', 'min', 'sparsity', 'histogram']):\n","        \"\"\" Attach a lot of summaries to a Tensor. \"\"\"\n","\n","        if name is None:\n","            # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n","            # session. This helps the clarity of presentation on tensorboard.\n","            name = re.sub('%s_[0-9]*/' % 'tower', '', tensor.name)\n","            name = re.sub(':', '-', name)\n","\n","        with tf.name_scope('summary_' + name):\n","            summaries = []\n","            if len(tensor.shape) == 0:\n","                summaries.append(tf.summary.scalar(name, tensor))\n","            else:\n","                if 'mean' in summary_type:\n","                    mean = tf.reduce_mean(tensor)\n","                    summaries.append(tf.summary.scalar(name + '/mean', mean))\n","                if 'stddev' in summary_type:\n","                    mean = tf.reduce_mean(tensor)\n","                    stddev = tf.sqrt(tf.reduce_mean(tf.square(tensor - mean)))\n","                    summaries.append(tf.summary.scalar(name + '/stddev', stddev))\n","                if 'max' in summary_type:\n","                    summaries.append(tf.summary.scalar(name + '/max', tf.reduce_max(tensor)))\n","                if 'min' in summary_type:\n","                    summaries.append(tf.summary.scalar(name + '/min', tf.reduce_min(tensor)))\n","                if 'sparsity' in summary_type:\n","                    summaries.append(tf.summary.scalar(name + '/sparsity', tf.nn.zero_fraction(tensor)))\n","                if 'histogram' in summary_type:\n","                    summaries.append(tf.summary.histogram(name, tensor))\n","            return tf.summary.merge(summaries)\n","\n","    if not isinstance(tensor_collection, (list, tuple, dict)):\n","        tensor_collection = [tensor_collection]\n","    with tf.name_scope('summaries'):\n","        summaries = []\n","        if isinstance(tensor_collection, (list, tuple)):\n","            for tensor in tensor_collection:\n","                summaries.append(_summary(tensor, None, summary_type))\n","        else:\n","            for tensor, name in tensor_collection.items():\n","                summaries.append(_summary(tensor, name, summary_type))\n","        return tf.summary.merge(summaries)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSjiy_b07le9"},"outputs":[],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Oct 23 13:57:32 2019\n","@author: yya\n","\"\"\"\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Fri Oct 11 18:31:42 2019\n","@author: siat-x\n","\"\"\"\n","\n","import os\n","import re\n","import math\n","import numpy as np\n","import tensorflow as tf\n","from datetime import datetime\n","import cv2\n","from glob import glob\n","from tensorflow.data.experimental import prefetch_to_device, shuffle_and_repeat, map_and_batch\n","from xml.etree import ElementTree\n","\n","\n","class GAN:\n","\n","    def __init__(self, sess, args):\n","        self.model_name = 'GAN'\n","        self.sess = sess\n","        self.gan_type = args.gan_type\n","        self.epoch = args.epoch\n","        self.decay_epoch = args.decay_epoch\n","        self.save_freq = args.save_freq\n","        self.iteration = args.iteration\n","        self.init_lr = args.lr\n","        self.beta1 = 0.5\n","        \n","        \"\"\" Dir \"\"\"\n","        self.checkpoint_dir = args.checkpoint_dir\n","        self.sample_dir = args.sample_dir\n","        self.result_dir = args.result_dir\n","        self.summaries_dir = args.summaries_dir\n","\n","        \"\"\" Weight \"\"\"\n","        self.rec_weight = args.rec_weight\n","        self.cls_weight = args.cls_weight\n","        self.ld = args.ld\n","        \n","        \"\"\" Channels \"\"\"\n","        self.batch_size = args.batch_size\n","        self.genn = args.n_gen\n","        self.disn = args.n_dis\n","        self.img_size = args.img_size\n","        self.img_ch = args.img_ch\n","        self.img_shape = (args.img_size, args.img_size, args.img_ch)\n","        self.ch = args.ch\n","            \n","        \"\"\" Dataset \"\"\"\n","        self.label_size = args.label_size\n","        self.augment_flag = args.augment_flag\n","        self.train_dataset_type = args.dataset_type\n","        self.train_dataset_img_type = args.dataset_img_type\n","        self.train_dataset_path = args.dataset_path\n","           \n","        \"\"\" Test \"\"\"\n","        self.test_path = args.test_path\n","        \n","##################################################################################\n","# Generator\n","##################################################################################\n","        \n","    def generator(self, x, c, reuse=False, train=True):\n","        ys = []\n","        channel = 256\n","        norm='batch'\n","        with tf.variable_scope('generator', reuse=reuse):\n","\n","            c = tf.cast(tf.reshape(c, shape=[-1, 1, 1, c.shape[-1]]), tf.float32)\n","            c = tf.tile(c, [1, x.shape[1], x.shape[2], 1])\n","            x = tf.concat([x, c], axis=-1)\n","            x = conv(x, channel, kernel=3, stride=1, pad=1, activation_fn='relu', is_training=train, norm_fn=norm, use_bias=False, scope='conv_b')\n","            \n","            for i in range(0, self.genn-2) :\n","                x = up_sample(x, scale_factor=2)\n","                x = conv(x, channel, kernel=5, stride=1, pad=2, activation_fn='relu', is_training=train, norm_fn=norm, use_bias=False, scope='deconv_'+str(i))\n","\n","            for i in range(self.genn-2, self.genn) :\n","                channel = channel/2\n","                x = up_sample(x, scale_factor=2)\n","                x = conv(x, channel, kernel=5, stride=1, pad=2, activation_fn='relu', is_training=train, norm_fn=norm, use_bias=False, scope='deconv_'+str(i))\n","                \n","            x = conv(x, self.img_ch, kernel=1, stride=1, pad=0, activation_fn='tanh', is_training=train, norm_fn='None', use_bias=False, scope='conv_x')\n","            \n","            return x\n","        \n","##################################################################################\n","# Discriminator\n","##################################################################################\n","\n","    def discriminator(self, x, reuse=False, train=True, dropout=False):\n","        norm='None'#'spectral#instance\n","        with tf.variable_scope(\"discriminator\", reuse=reuse) :\n","        \n","            channel = self.ch\n","            for i in range(0, self.disn):\n","                x = conv(x, channel, kernel=4, stride=2, pad=1, activation_fn='leaky', \n","                         is_training=train, norm_fn=norm, scope='conv_' + str(i))\n","                channel = channel * 2 if channel\u003c512 else 512\n","\n","            logit =  conv(x, 1, 3, stride=1, pad=1, activation_fn='None', \n","                          is_training=train, norm_fn='None', use_bias=False, scope='D_logit')\n","\n","            x = conv(x, self.label_size, kernel=int(x.shape[1]), stride=1, pad=0, activation_fn='None', \n","                     is_training=train, norm_fn='None', use_bias=False, scope='D_label')\n","            \n","            x = tf.reshape(x, shape=[-1, self.label_size])\n","\n","            return logit, x\n","        \n","##################################################################################\n","# Model\n","##################################################################################\n","\n","    def gradient_panalty(self, real, fake, logit_real, logit_fake, scope=\"discriminator\"):\n","\n","        if self.gan_type == 'wgan-div':\n","            grad_real = tf.gradients(logit_real, real)[0]\n","            grad_fake = tf.gradients(logit_fake, fake)[0]\n","            grad_real_norm = tf.norm(tf.layers.flatten(grad_real), axis=1) # l2 norm\n","            grad_fake_norm = tf.norm(tf.layers.flatten(grad_fake), axis=1) # l2 norm\n","            GP = tf.reduce_mean(tf.pow(grad_real_norm, 3)+tf.pow(grad_fake_norm, 3))\n","            return GP\n","            \n","        if self.gan_type == 'dragan' :\n","            shape = tf.shape(real)\n","            eps = tf.random_uniform(shape=shape, minval=0., maxval=1.)\n","            x_mean, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n","            x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\n","            noise = 0.5 * x_std * eps  # delta in paper\n","            alpha = tf.random_uniform(shape=[shape[0], 1, 1, 1], minval=-1., maxval=1.)\n","            interpolated = tf.clip_by_value(real + alpha * noise, -1., 1.)  # x_hat should be in the space of X\n","        else :\n","            shape = tf.shape(real)\n","            alpha = tf.random_uniform(shape=[shape[0], 1, 1, 1], minval=0., maxval=1.)\n","            interpolated = alpha*real + (1. - alpha)*fake\n","\n","        logit, _ = self.discriminator(interpolated, reuse=True)\n","\n","        GP = 0\n","\n","        grad = tf.gradients(logit, interpolated)[0] # gradient of D(interpolated)\n","        grad_norm = tf.norm(tf.layers.flatten(grad), axis=1) # l2 norm\n","\n","        # WGAN - LP\n","        if self.gan_type == 'wgan-lp' :\n","            GP = tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n","\n","        if self.gan_type == 'wgan-gp' or self.gan_type == 'dragan':\n","            GP = tf.reduce_mean(tf.square(grad_norm - 1.))\n","\n","        return GP\n","\n","    def optimizer_graph_generator(self, gen_loss, dis_loss, learning_rate_g, learning_rate_d, beta1):\n","        train_vars = tf.trainable_variables()\n","        gen_vars = [var for var in train_vars if var.name.startswith('generator')]\n","        dis_vars = [var for var in train_vars if var.name.startswith('discriminator')]\n","        # optimizer\n","        gen_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_g,beta1=beta1,beta2=0.999).minimize(gen_loss, var_list=gen_vars)\n","        dis_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_d,beta1=beta1,beta2=0.999).minimize(dis_loss, var_list=dis_vars)\n","        return gen_optimizer, dis_optimizer\n","    \n","    def build_model(self):\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","        self.input = tf.placeholder(tf.float32, shape=[None, 4, 4, 64], name='input')\n","        self.lr_g = tf.placeholder(tf.float32, name='lr_g')\n","        self.lr_d = tf.placeholder(tf.float32, name='lr_d')\n","        \"\"\" Dataset \"\"\"\n","        self.Image_Data = ImageData(self.train_dataset_path, img_shape = self.img_shape, augment_flag = self.augment_flag, \n","                                    data_type = self.train_dataset_type, img_type = self.train_dataset_img_type, label_size=self.label_size)\n","\n","        trainA = tf.data.Dataset.from_tensor_slices((self.Image_Data.train_dataset, self.Image_Data.train_label))\n","\n","        dataset_num = len(self.Image_Data.train_dataset)\n","        gpu_device = '/gpu:0'\n","        trainA = trainA.\\\n","            apply(shuffle_and_repeat(dataset_num)).\\\n","            apply(map_and_batch(self.Image_Data.image_processing, self.batch_size, num_parallel_batches=8, drop_remainder=True)).\\\n","            apply(prefetch_to_device(gpu_device, self.batch_size))\n","\n","        trainA_iterator = trainA.make_one_shot_iterator()\n","\n","        self.real_imgs, self.label_o = trainA_iterator.get_next()\n","        \n","        \"\"\" generation \"\"\"\n","        self.fake_imgs = self.generator(self.input, self.label_o)\n","                \n","        \"\"\" Discriminator for real \"\"\"\n","        real_logits, real_label = self.discriminator(self.real_imgs)\n","        \n","        \"\"\" Discriminator for fake \"\"\"\n","        fake_logits, fake_label = self.discriminator(self.fake_imgs, reuse=True)\n","        \n","        \"\"\" Define Loss \"\"\"\n","        if self.gan_type.__contains__('wgan') or self.gan_type == 'dragan' :\n","            grad_pen = self.gradient_panalty(self.real_imgs, self.fake_imgs, real_logits, fake_logits)\n","        else :\n","            grad_pen = 0\n","        \n","        g_cls_loss = classification_loss2(logit=fake_label, label=self.label_o)\n","        d_cls_loss = classification_loss2(logit=real_label, label=self.label_o)\n","        \n","        dis_loss = discriminator_loss(self.gan_type, real_logits, fake_logits) + self.ld * grad_pen\n","        gen_loss = generator_loss(self.gan_type, fake_logits)\n","        \n","        D_loss = dis_loss + self.cls_weight * d_cls_loss\n","        G_loss = gen_loss + self.cls_weight * g_cls_loss\n","        \"\"\" Optimizer \"\"\"\n","        D_loss += regularization_loss('discriminator')\n","        G_loss += regularization_loss('generator')\n","        self.gen_optimizer, self.dis_optimizer = self.optimizer_graph_generator(G_loss, D_loss, self.lr_g, self.lr_d, self.beta1)\n","        \"\"\" Summaries \"\"\"\n","        self.g_summary = summary({G_loss:'G_loss',\n","                                  gen_loss: 'gen_loss',\n","                                  g_cls_loss:'g_cls_loss'})\n","        self.d_summary = summary({D_loss:'D_loss',\n","                                  dis_loss: 'dis_loss', \n","                                  d_cls_loss:'d_cls_loss'})\n","        \n","    def train(self):\n","        \n","        self.sess.run(tf.global_variables_initializer())\n","        self.saver = tf.train.Saver()\n","    \n","        # summary writer\n","        self.writer = tf.summary.FileWriter(os.path.join(self.summaries_dir), self.sess.graph)\n","        \n","        # restore check-point if it exits\n","        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n","        if could_load:\n","            start_epoch = (int)(checkpoint_counter / self.iteration)\n","            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n","            step = checkpoint_counter\n","            print(\" [*] Load SUCCESS\")\n","        else:\n","            if not os.path.exists(self.checkpoint_dir):\n","                os.makedirs(self.checkpoint_dir)\n","            start_epoch = 0\n","            start_batch_id = 0\n","            step = 1\n","            print(\" [!] Load failed...\")\n","        self.variables_count()\n","                \n","        lr = self.init_lr\n","        for epoch in range(start_epoch, self.epoch):\n","            lr = self.init_lr if epoch \u003c self.decay_epoch else self.init_lr * (self.epoch - epoch) / (self.epoch - self.decay_epoch) # linear decay\n","    \n","            for idx in range(start_batch_id, self.iteration):\n","                input_z = np.random.uniform(-1, 1, size=(self.batch_size,4,4,64))\n","                train_feed_dict = {self.input : input_z, self.lr_g : lr, self.lr_d : lr}\n","                    \n","                _,d_summary_opt = self.sess.run([self.dis_optimizer, self.d_summary], feed_dict = train_feed_dict)\n","                self.writer.add_summary(d_summary_opt, step)\n","                if (step-1)%5==0:\n","                    _,g_summary_opt = self.sess.run([self.gen_optimizer, self.g_summary], feed_dict = train_feed_dict)\n","                    self.writer.add_summary(g_summary_opt, step)\n","                step += 1\n","                    \n","                if np.mod(idx + 1, self.save_freq) == 0:\n","                    samples_a, samples_t = self.sess.run([self.real_imgs,self.fake_imgs], feed_dict = train_feed_dict)\n","                    test_shape = (self.batch_size*self.img_size, self.img_size, self.img_ch)\n","                    samples_a=np.uint8(127.5*(np.reshape(samples_a,test_shape)+1.0))\n","                    samples_t=np.uint8(127.5*(np.reshape(samples_t,test_shape)+1.0))\n","                    sample = np.concatenate([samples_a,samples_t],axis=1)#,samples_p\n","                    sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR) \n","                    cv2.imwrite(os.path.join(self.sample_dir,str(step)+'.jpg'), sample)\n","        \n","                    print(datetime.now().strftime('%c'), ' epoch:', epoch, ' idx:', idx, '/', self.iteration, ' step:', step)\n","                    self.saver.save(self.sess, os.path.join(self.checkpoint_dir, self.model_name + '.model'), global_step=step)\n","\n","            # After an epoch, start_batch_id is set to zero\n","            # non-zero value is only for the first epoch after loading pre-trained model\n","            start_batch_id = 0\n","    \n","            # save model for final step\n","            self.saver.save(self.sess, os.path.join(self.checkpoint_dir, self.model_name + '.model'), global_step=step)\n","\n","    def test(self):\n","        \n","        self.label = tf.placeholder(tf.float32, [1, self.label_size], name='label')\n","        self.output = self.generator(self.input, self.label, reuse=True, train=False)\n","        \n","        # restore check-point if it exits\n","        self.saver = tf.train.Saver()\n","        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n","        if could_load:\n","            print(\" [*] Load SUCCESS\")\n","        else:\n","            print(\" [!] Load failed...\")\n","\n","        class_names = ['D20', 'D10', 'D40', 'D00']\n","        gov_class = ['Czech', 'India', 'Japan']\n","        for gov in gov_class:\n","            file_list = os.listdir( self.test_path + gov + '/annotations/xmls')\n","\n","            for file in file_list:\n","\n","                im_name = file.split('.')[0] + '.jpg'\n","\n","                full_impath =  self.test_path + gov + '/images/' + im_name\n","\n","                save_path =  self.test_path + gov + '/images_x/'\n","                if not os.path.exists(save_path):\n","                    os.makedirs(save_path)\n","\n","                infile_xml = open( self.test_path + gov + '/annotations/xmls/' + file, encoding='utf-8')\n","                tree = ElementTree.parse(infile_xml)\n","                root = tree.getroot()\n","                \n","                cnt = 0\n","                flag = 0\n","                for obj in root.iter('object'):\n","                    cls_name = obj.find('name').text\n","                    flag = 0\n","                    for n in class_names:\n","                        if cls_name == n:\n","                            flag+=1\n","                    if flag==0:\n","                        continue\n","                    xmlbox = obj.find('bndbox')\n","                    xmin = int(xmlbox.find('xmin').text)\n","                    xmax = int(xmlbox.find('xmax').text)\n","                    ymin = int(xmlbox.find('ymin').text)\n","                    ymax = int(xmlbox.find('ymax').text)\n","\n","                    if xmin\u003exmax:\n","                        xmin = int(xmlbox.find('xmax').text)\n","                        xmax = int(xmlbox.find('xmin').text)\n","\n","                    if ymin\u003eymax:\n","                        ymin = int(xmlbox.find('ymax').text)\n","                        ymax = int(xmlbox.find('ymin').text)\n","\n","                    # open image\n","                    img = cv2.imread(full_impath)\n","\n","                    label = np.reshape(self.Image_Data.get_class_one_hot(cls_name), [1, self.label_size]) \n","                    input = np.random.uniform(-1, 1, size=(1,4,4,64))\n","                    img_o = self.sess.run([self.output], feed_dict={self.input: input, self.label: label})#\n","                    img_o = np.uint8(127.5*(np.reshape(img_o,(self.img_size, self.img_size, 3))+1.0))\n","                    img_o = cv2.cvtColor(img_o, cv2.COLOR_RGB2BGR) \n","                    img_o = cv2.resize(img_o, (xmax-xmin, ymax-ymin))\n","                    width, height, channels = img.shape\n","                    # img_b = cv2.GaussianBlur(img,(5,5),0)\n","                    # img[ymin:ymax, xmin:xmax] = img_b[ymin:ymax, xmin:xmax]\n","                    obj = np.zeros_like(img)\n","                    obj[ymin:ymax, xmin:xmax] = img_o\n","                    mask = np.zeros_like(img)\n","                    mask[ymin:ymax, xmin:xmax] = 255\n","                    center = (xmin+(xmax-xmin)//2, ymin+(ymax-ymin)//2)\n","                    img = cv2.seamlessClone(obj, img, mask, center, cv2.MIXED_CLONE)\n","                if flag\u003e0:\n","                    save_impath = save_path + im_name\n","                    cv2.imwrite(save_impath, img)\n","                    # cv2.imshow('img',mask)\n","                    # cv2.waitKey(0)\n","        \n","\n","    def load(self, checkpoint_dir):\n","        print(\" [*] Reading checkpoints...\")\n","#        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n","\n","        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n","        if ckpt and ckpt.model_checkpoint_path:\n","            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n","            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n","            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n","            print(\" [*] Success to read {}\".format(ckpt_name))\n","            return True, counter\n","        else:\n","            print(\" [*] Failed to find a checkpoint\")\n","            return False, 0\n","        \n","    def set_value(self, matrix, x, y, val):\n","        w = int(matrix.get_shape()[0])\n","        h = int(matrix.get_shape()[1])\n","        mult_matrix = tf.sparse_tensor_to_dense(tf.SparseTensor(indices=[[x, y]], values=[-1.0], dense_shape=[w, h])) + 1.0\n","        diff_matrix = tf.sparse_tensor_to_dense(tf.SparseTensor(indices=[[x, y]], values=[val], dense_shape=[w, h]))\n","        matrix = tf.multiply(matrix, mult_matrix) \n","        matrix = matrix + diff_matrix\n","        return matrix\n","    \n","    def variables_count(self):\n","        train_vars = tf.trainable_variables()\n","        gen_vars = [var for var in train_vars if var.name.startswith('generator')]\n","        dis_vars = [var for var in train_vars if var.name.startswith('discriminator')]\n","        print(\"Generator variables:\", np.sum([np.prod(v.get_shape().as_list()) for v in gen_vars]))\n","        print(\"Discriminator variables:\", np.sum([np.prod(v.get_shape().as_list()) for v in dis_vars]))\n","        print(\"Total variables:\", np.sum([np.prod(v.get_shape().as_list()) for v in train_vars]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3l5Ho8V6_LOH"},"outputs":[],"source":["!mkdir -p IEEEbigdata2020/train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urSfPmkw-6EU"},"outputs":[],"source":["!mv -v train/* IEEEbigdata2020/train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vFWaEYdj-dF_"},"outputs":[],"source":["\n","# coding: UTF-8\n","import cv2\n","import os\n","from xml.etree import ElementTree\n","\n","\n","data_path = './IEEEbigdata2020/'\n","for gov in ['Czech', 'India', 'Japan']:\n","    file_list = os.listdir(data_path + 'train/' + gov + '/annotations/xmls')\n","\n","    for file in file_list:\n","\n","        im_name = file.split('.')[0] + '.jpg'\n","\n","        full_impath = data_path + 'train/' + gov + '/images/' + im_name\n","\n","        infile_xml = open(data_path + 'train/' + gov + '/annotations/xmls/' + file, encoding='utf-8')\n","        tree = ElementTree.parse(infile_xml)\n","        root = tree.getroot()\n","        \n","        cnt = 0\n","        for obj in root.iter('object'):\n","            \n","            cls_name = obj.find('name').text\n","            xmlbox = obj.find('bndbox')\n","            \n","            \n","            xmin = int(xmlbox.find('xmin').text)\n","            xmax = int(xmlbox.find('xmax').text)\n","            ymin = int(xmlbox.find('ymin').text)\n","            ymax = int(xmlbox.find('ymax').text)\n","\n","            if xmin\u003exmax:\n","                xmin = int(xmlbox.find('xmax').text)\n","                xmax = int(xmlbox.find('xmin').text)\n","\n","            if ymin\u003eymax:\n","                ymin = int(xmlbox.find('ymax').text)\n","                ymax = int(xmlbox.find('ymin').text)\n","\n","            # open image\n","            img = cv2.imread(full_impath)\n","\n","            \n","            # crop bounding box\n","            roi = img[ymin:ymax, xmin:xmax]\n","\n","            if roi.sum() != 0:\n","                # resize image\n","                h = 128\n","                w = 128\n","                roiResized = cv2.resize(roi, (h, w))\n","\n","\n","                im_name = im_name.split('.')[0] + '_' + str(cnt) + '.png'\n","               # im_name = im_name.split('.')[0] + '.png'\n","\n","                # save resized image\n","                save_dir = data_path + 'data/' + cls_name\n","                if not os.path.exists(save_dir):\n","                    os.makedirs(save_dir)\n","                cv2.imwrite(save_dir + '/' + im_name, roiResized)\n","\n","                cnt = cnt + 1\n","                 \n","                im_name = file.split('.')[0] + '.jpg'\n","                "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO9XH3kknk4RF+JgKJrsrjz","background_execution":"on","machine_shape":"hm","name":"GAN .ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}